{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from data import NoisyBSDSDataset\n",
    "from argument import Args\n",
    "from model import DnCNN, UDnCNN, DUDnCNN\n",
    "import nntools as nt\n",
    "from utils import DenoisingStatsManager, plot\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as td\n",
    "import torch.quantization.quantize_fx as quantize_fx\n",
    "from torch.quantization.fuse_modules import fuse_known_modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "from PIL import Image\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyBSDSDataset(td.Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, mode='train', image_size=(180, 180), sigma=30):\n",
    "        super(NoisyBSDSDataset, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.image_size = image_size\n",
    "        self.sigma = sigma\n",
    "        self.images_dir = os.path.join(root_dir, mode)\n",
    "        self.files = os.listdir(self.images_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"NoisyBSDSDataset(mode={}, image_size={}, sigma={})\". \\\n",
    "            format(self.mode, self.image_size, self.sigma)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.images_dir, self.files[idx])\n",
    "        clean = Image.open(img_path).convert('RGB')   \n",
    "        # random crop\n",
    "        i = np.random.randint(clean.size[0] - self.image_size[0])\n",
    "        j = np.random.randint(clean.size[1] - self.image_size[1])\n",
    "        \n",
    "        clean = clean.crop([i, j, i+self.image_size[0], j+self.image_size[1]])\n",
    "        transform = tv.transforms.Compose([\n",
    "            # convert it to a tensor\n",
    "            tv.transforms.ToTensor(),\n",
    "            # normalize it to the range [âˆ’1, 1]\n",
    "            tv.transforms.Normalize((.5, .5, .5), (.5, .5, .5))\n",
    "            ])\n",
    "        clean = transform(clean)\n",
    "        \n",
    "        noisy = clean + 2 / 255 * self.sigma * torch.randn(clean.shape)\n",
    "        return noisy, clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    image = (image + 1) / 2\n",
    "    image[image < 0] = 0\n",
    "    image[image > 1] = 1\n",
    "    h = ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, D=6, C=64, device=torch.device('cpu')):\n",
    "    net = model.DUDnCNN(D, C)\n",
    "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "    net.load_state_dict(checkpoint['Net'])\n",
    "    net.eval()\n",
    "\n",
    "    return net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_tensor(img, device):\n",
    "    tensor = torch.FloatTensor(img).to(device)\n",
    "    tensor = tensor.permute([2, 0, 1]) / 255.\n",
    "    tensor = (tensor - 0.5) / 0.5\n",
    "\n",
    "    return tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_img(tensor):\n",
    "    tensor = tensor[0].permute([1, 2, 0])\n",
    "    tensor = (tensor * 0.5 + 0.5) * 255\n",
    "    tensor = tensor.clamp(0, 255)\n",
    "    return tensor.cpu().numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(quantize_type, model, input_example=None):\n",
    "    if quantize_type == 'dynamic':\n",
    "        model = torch.quantization.quantize_dynamic(\n",
    "            model,\n",
    "            {torch.nn.Conv2d},\n",
    "            dtype=torch.qint8\n",
    "        )\n",
    "    elif quantize_type == 'static':\n",
    "        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "        for i in range(len(model.bn)):\n",
    "            conv, bn = model.conv[i+1], model.bn[i]\n",
    "            conv_new, bn_new = fuse_known_modules([conv, bn])\n",
    "            setattr(model.conv, str(i+1), conv_new)\n",
    "            setattr(model.bn, str(i), bn_new)\n",
    "        model_fp32_fused = model\n",
    "        model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "        if input_example is not None:\n",
    "            model_fp32_prepared(input_example)\n",
    "        model = torch.quantization.convert(model_fp32_prepared)\n",
    "    elif quantize_type == 'fx_dynamic':\n",
    "        qconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}\n",
    "        # prepare\n",
    "        model_prepared = quantize_fx.prepare_fx(model, qconfig_dict)\n",
    "        # no calibration needed when we only have dynamici/weight_only quantization\n",
    "        # quantize\n",
    "        model = quantize_fx.convert_fx(model_prepared)\n",
    "    elif quantize_type == 'fx_static':\n",
    "        # qconfig_dict = {\"\": torch.quantization.get_default_qconfig('qnnpack')}\n",
    "        qconfig_dict = {\"\": torch.quantization.get_default_qconfig('fbgemm')}\n",
    "        # prepare\n",
    "        model_prepared = quantize_fx.prepare_fx(model, qconfig_dict)\n",
    "        # calibrate (not shown)\n",
    "        if input_example is not None:\n",
    "            model_prepared(input_example)\n",
    "        # quantize\n",
    "        model = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(img_set,model_path,quantize='none'):\n",
    "    args = Args()\n",
    "    args.quantize = quantize\n",
    "    device = 'cpu'\n",
    "    denoise = load_model(model_path, args.D, args.C, device=device)\n",
    "\n",
    "    img = []\n",
    "    titles = ['clean', 'noise', 'denoise']\n",
    "    x, clean = img_set\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    img.append(clean)\n",
    "    img.append(x[0])\n",
    "\n",
    "    if args.quantize:\n",
    "        print('Quantize model...'+quantize)\n",
    "        denoise = quantize_model(args.quantize, denoise, input_example=x)\n",
    "\n",
    "    t = time.time()\n",
    "    with torch.no_grad():\n",
    "        y = denoise(x)\n",
    "    img.append(y[0])\n",
    "\n",
    "    print(f'Elapsed: {(time.time() - t) * 1000:.2f}ms')\n",
    "    print(f'Image size is {x[0].shape}.')\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=3, figsize=(9,5), sharex='all', sharey='all')\n",
    "    for i in range(len(img)):\n",
    "        myimshow(img[i], ax=axes[i])\n",
    "        axes[i].set_title(f'{titles[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = os.environ.get('DATA_DIR')+'/images'\n",
    "test_set = NoisyBSDSDataset(dataset_root_dir, mode='test', image_size=(320, 320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_set = test_set[0]\n",
    "path = os.environ.get('TRAINING_DIR')+'/checkpoint/checkpoint.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(img_set,path,'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(img_set,path,'static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(img_set,path,'dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(img_set,path,'fx_static')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(img_set,path,'fx_dynamic')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
