{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from data import NoisyBSDSDataset\n",
    "from argument import Args\n",
    "from model import DnCNN, UDnCNN, DUDnCNN\n",
    "import nntools as nt\n",
    "from utils import DenoisingStatsManager, plot\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.quantization.quantize_fx as quantize_fx\n",
    "from torch.quantization.fuse_modules import fuse_known_modules\n",
    "\n",
    "import model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-D', default=6, type=int)\n",
    "    parser.add_argument('-C', default=64, type=int)\n",
    "    parser.add_argument('--image')\n",
    "    parser.add_argument('--output')\n",
    "    parser.add_argument(\n",
    "        '--quantize',\n",
    "        choices=[None, 'dynamic', 'static', 'fx_dynamic', 'fx_static'],\n",
    "        default=None\n",
    "    )\n",
    "    parser.add_argument('--show', action='store_true')\n",
    "    parser.add_argument('--model', required=True)\n",
    "\n",
    "    return parser.parse_args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def load_model(path, D=6, C=64, device=torch.device('cpu')):\n",
    "    net = model.DUDnCNN(D, C)\n",
    "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "    net.load_state_dict(checkpoint['Net'])\n",
    "    net.eval()\n",
    "\n",
    "    return net.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def img_to_tensor(img, device):\n",
    "    tensor = torch.FloatTensor(img).to(device)\n",
    "    tensor = tensor.permute([2, 0, 1]) / 255.\n",
    "    tensor = (tensor - 0.5) / 0.5\n",
    "\n",
    "    return tensor.unsqueeze(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def tensor_to_img(tensor):\n",
    "    tensor = tensor[0].permute([1, 2, 0])\n",
    "    tensor = (tensor * 0.5 + 0.5) * 255\n",
    "    tensor = tensor.clamp(0, 255)\n",
    "    return tensor.cpu().numpy().astype(np.uint8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def quantize_model(quantize_type, model, input_example=None):\n",
    "    if quantize_type == 'dynamic':\n",
    "        model = torch.quantization.quantize_dynamic(\n",
    "            model,\n",
    "            {torch.nn.Conv2d},\n",
    "            dtype=torch.qint8\n",
    "        )\n",
    "    elif quantize_type == 'static':\n",
    "        model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "        for i in range(len(model.bn)):\n",
    "            conv, bn = model.conv[i+1], model.bn[i]\n",
    "            conv_new, bn_new = fuse_known_modules([conv, bn])\n",
    "            setattr(model.conv, str(i+1), conv_new)\n",
    "            setattr(model.bn, str(i), bn_new)\n",
    "        model_fp32_fused = model\n",
    "        model_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n",
    "        if input_example is not None:\n",
    "            model_fp32_prepared(input_example)\n",
    "        model = torch.quantization.convert(model_fp32_prepared)\n",
    "    elif quantize_type == 'fx_dynamic':\n",
    "        qconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}\n",
    "        # prepare\n",
    "        model_prepared = quantize_fx.prepare_fx(model, qconfig_dict)\n",
    "        # no calibration needed when we only have dynamici/weight_only quantization\n",
    "        # quantize\n",
    "        model = quantize_fx.convert_fx(model_prepared)\n",
    "    elif quantize_type == 'fx_static':\n",
    "        # qconfig_dict = {\"\": torch.quantization.get_default_qconfig('qnnpack')}\n",
    "        qconfig_dict = {\"\": torch.quantization.get_default_qconfig('fbgemm')}\n",
    "        # prepare\n",
    "        model_prepared = quantize_fx.prepare_fx(model, qconfig_dict)\n",
    "        # calibrate (not shown)\n",
    "        if input_example is not None:\n",
    "            model_prepared(input_example)\n",
    "        # quantize\n",
    "        model = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def main():\n",
    "    args = parse_args()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    denoise = load_model(args.model, args.D, args.C, device=device)\n",
    "\n",
    "    img = cv2.cvtColor(cv2.imread(args.image), cv2.COLOR_BGR2RGB)\n",
    "    small = cv2.resize(img, (720, 720), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    tensor = img_to_tensor(small, device)\n",
    "\n",
    "    if args.quantize:\n",
    "        print('Quantize model...')\n",
    "        denoise = quantize_model(args.quantize, denoise, input_example=tensor)\n",
    "        print('Done.')\n",
    "\n",
    "    t = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = denoise(tensor)\n",
    "\n",
    "    print(f'Elapsed: {(time.time() - t) * 1000:.2f}ms')\n",
    "    output = tensor_to_img(output)\n",
    "    combined = np.hstack([small, output])\n",
    "\n",
    "    if args.show:\n",
    "        cv2.imshow('Image', combined[:, :, ::-1])\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    if args.output:\n",
    "        cv2.imwrite(args.output, combined[:, :, ::-1])\n",
    "        print(f'Image saved to {args.output}.')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "args = ArgsQ()\n",
    "args.plot = True\n",
    "vars(args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}